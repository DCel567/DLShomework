

def logit(x, w):
	return np.dot(x, w)


def sigmoid(h):
	return 1. / (1 + np.exp(-h))


class MyLogisticRegression(object):
	def __init__(self):
		self.w = None

	def fit(self, X, y, epochs=10, lr=0.1, batch_size=100):
		n, k = X.shape
		if self.w is None:
			np.random.seed(42)
			# Вектор столбец в качестве весов
			self.w = np.random.randn(k + 1)

		X_train = np.concatenate((np.ones((n, 1)), X), axis=1)

		losses = []

		# Положите в лист losses лосс на каждом батче. Не нужно усреднять лосс по эпохе.

		for i in range(epochs):
			for X_batch, y_batch in generate_batches(X_train, y, batch_size):
				# В X_train уже добавлен вектор 1

				predictions = self._predict_proba_internal(X_batch)
				loss = self.__loss(y_batch, predictions)

				assert (np.array(loss).shape == tuple()), "Лосс должен быть скаляром!"

				losses.append(loss)

				# YOUR CODE: обновите self.w по формуле градиентного спуска.
				# Используйте функцию self.get_grad для вычисления градиента. Не забудьте про learning rate!
				self.w -= lr * self.get_grad(X_batch, y_batch, predictions)

		return losses

	def get_grad(self, X_batch, y_batch, predictions):
		"""

		param X_batch: np.array[batch_size, n_features + 1] --- матрица объекты-признаки
		param y_batch: np.array[batch_size] --- батч целевых переменных
		param predictions: np.array[batch_size] --- батч вероятностей классов

		Принимает на вход X_batch с уже добавленной колонкой единиц.
		Выдаёт градиент функции потерь в логистической регрессии
		как сумму градиентов функции потерь на всех объектах батча
		ВНИМАНИЕ! Нулевая координата вектора весов -- это BIAS, а не вес признака.
		Также не нужно ДЕЛИТЬ ГРАДИЕНТ НА РАЗМЕР БАТЧА:
		нас интересует не среднее, а сумма.
		В качестве оператора умножения матриц можно использовать @

		Выход -- вектор-столбец градиентов для каждого веса (np.array[n_features + 1])
		"""

		# компонент градиента из логрегрессии
		# следите за размерностями

		grad_basic = X_batch.T @ (predictions - y_batch)
		assert grad_basic.shape == (X_batch.shape[1],), "Градиенты должны быть столбцом из k_features + 1 элементов"

		return grad_basic

	def predict_proba(self, X):
		n, k = X.shape
		X_ = np.concatenate((np.ones((n, 1)), X), axis=1)
		return sigmoid(logit(X_, self.w))

	def _predict_proba_internal(self, X):
		"""
		Возможно, вы захотите использовать эту функцию вместо predict_proba, поскольку
		predict_proba конкатенирует вход с вектором из единиц, что не всегда удобно
		для внутренней логики вашей программы
		"""
		return sigmoid(logit(X, self.w))

	def predict(self, X, threshold=0.5):
		return self.predict_proba(X) >= threshold

	def get_weights(self):
		return self.w.copy()

	# copy тут используется неспроста. Если copy не использовать, то get_weights()
	# выдаст ссылку на объект, а, значит, модифицируя результат применения функции
	# get_weights(), вы модифицируете и веса self.w. Если вы хотите модифицировать веса,
	# (например, в fit), используйте self.w

	def __loss(self, y, p):
		p = np.clip(p, 1e-10, 1 - 1e-10)
		return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))